####################################################
#############采集任务集群配置，必须配置项###########
####################################################

##本机IP【集群唯一，必须是IP不能是别名，因为http服务不识别】。默认来自java.net.InetAddress.getCanonicalHostName()
host.name=localhost

##zookeeper连接URL，多个用逗号分割
zk.connect=hnode01:2181,hnode02:2181,hnode03:2181

##zk交互时，使用的跟目录(集群所有机器必须配置一样)
zk.base.node=collect

##提供给外部服务的端口配置
collect.server.http.port=7892
collect.server.ws.port=7893
collect.server.socket.port=7894

##############################################
#########下面是非必须配置项，都有默认值#######
##############################################

##是否提供UI服务，默认true。ui服务可以通过浏览器查看集群运行情况
ui.enable=true
ui.port=7891

##zk连接超时
zk.connect.timeout.ms=30000
##zk会话超时(6秒)
zk.session.timeout.ms=8000


##缓存hdfs路径（保证数据不丢失）——也做为本地缓存目录
##存储规则TXT类型，一行一条消息。文件：${collect.cache.dir}/${topic_name}/collect.cache
collect.cache.dir=/hadoop/data1/collect/cache

##发送线程数
collect.send.threads=3

##每个主题发送队列缓存数(按每条消息500b算，合计大概5M)
collect.send.queue.size=10000

##存储队列上限，当达到此值将刷新到hdfs（10M）
collect.cache.max.size=10485760

##存储队列刷新时间间隔，将数据刷新到hdfs
collect.cache.interval.ms=5000

##排重过滤缓存数据目录
collect.distinct.dir=/hadoop/data1/collect/distinct

##采集明细日志目录
collect.log.path=logs

##明细日志记录频率
collect.detail.log.saved=false
detail.log.write.interval.ms=30000
##异常日志记录频率
error.log.write.interval.ms=5000
##发送kafka日志记录频率
send.log.write.interval.ms=5000
##统计日志记录频率
total.log.write.interval.ms=10000

######################################################
################下面是系统通用配置项##################
##元数据存储库
db.url=jdbc:mysql://hnode01:3306/estorm?autoReconnect=true
db.user=root
db.pass=Jvm2012
db.max.wait=60000
db.max.active=50
ds.id=1

##日志存放目录 
log.url=logs
##ALL所有，DEBUG<INFO<WARN<ERROR<FATAL
log.level=DEBUG
log.fileName=collect
##日志文件循环规则【none不循环，day按天，hour按小时，min按分钟，month按月】，默认按天生成日志
log.loop.rule=day
###################################
#######################################################

